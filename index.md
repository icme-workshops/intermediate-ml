---
layout: default
---

## Workshop Description

Through a series of rapid surveys, we will present an overview of recent topics in deep learning and machine learning with particular relevance for practitioners. Areas will include SVMs and kernels, variational autoencoders (VAEs) and dimensionality reduction, transfer learning, representation learning, and weakly supervised / semi-supervised / self-supervised learning. This workshop will assume a familiarity with basic concepts from both machine learning and deep learning as taught in the introductory workshops on those topics, but it will not assume a deep statistical background. Prior exposure to neural networks is highly recommended.

## About the Instructors

![Sherrie Wang](/assets/img/sherrie.png){:style="max-width:30%;"}

Sherrie Wang graduated from Stanford in 2021 with a PhD in Computational and Mathematical Engineering and is now a Ciriacy-Wantrup Postdoctoral Fellow at UC Berkeley. She works on developing machine learning methods for remote sensing applications, especially in settings where ground truth labels are scarce. These methods are then applied to problems in sustainable agriculture and development, such as mapping where crops are grown in developing countries.

![Alexander Ioannidis](/assets/img/alex.png){:style="max-width:30%;"}

Alexander Ioannidis earned his Ph.D. in Computational and Mathematical Engineering and Masters in Management Science and Engineering both at Stanford University. He is a research fellow working on developing novel machine learning techniques for medical and genomic applications in the Department of Biomedical Data Science at Stanford. Prior to this he earned a bachelors in Chemistry and Physics from Harvard, an M.Phil from the University of Cambridge and conducted research for several years on novel superconducting and quantum computing architectures. In his free time, he enjoys sailing.

## Pre-workshop Checklist

1. Sign up for [Piazza](http://piazza.com/icme/summer2021/icme12) (class code *icme*). We will be using Piazza to answer questions during the workshop.
2. You should have received a welcome email with the Zoom link and password.  Please email us (tanishj [at] stanford [dot] edu) if you haven't.
3. Familiarize yourself with the schedule and see you Monday August 16th at 8:00 am PT!

## Schedule and Slides

#### Monday August 16th

- Session 1 (8:00 AM to 9:30 AM) [[slides](/docs/intro.pdf)]
  - Introduction: Intermediate Topics in ML
  -  Additional resources: Excellent [free textbook](https://www.statlearning.com/) with code examples (SVM example 9.6.2, ROC example 9.6.3); great free Convex Optimization [textbook](https://web.stanford.edu/~boyd/cvxbook/)
- Session 2 (9:30 AM to 11:00 AM)  [[slides](/docs/transfer-learning_label-generation.pdf)]
  - Transfer Learning
  - Generating Labels for Deep Learning

#### Tuesday August 17th

- Session 3 (8:00 AM to 9:30 AM)
  - Dimensionality Reduction
  - Variational Autoencoders (VAEs)
- Session 4 (9:30 AM to 11:00 AM) [[slides](/docs/representation-learning_weak-supervision.pdf)]
  - Representation Learning and Self-Supervised Learning
  - Weakly Supervised Learning

<!-- ## Slides

* Session 1 - [slides](/docs/dlworkshop2021_1.pdf)
* Session 2 - [slides](/docs/dlworkshop2021_2.pdf)

## Jupyter Notebooks 

Below links should open the notebooks in [Google Collaboratory](https://colab.research.google.com/), after they open you may have to click "Open in Playground" to be able to run code.

*Links coming soon!*

* [TFWalkthrough.ipynb](https://colab.research.google.com/drive/1yGCtmXoN-bvFpOvcwxE5TJ2lu4WSyPAB)
* [KerasWalkthrough.ipynb](https://colab.research.google.com/drive/1uX27nH7K7UUn0RoQ0mREZ6FSiTv7F4TJ)
* [TransferLearning.ipynb](https://colab.research.google.com/drive/1QrNPyIalL4_i8aMO6426GV40dk3anPwJ) 


## Additional Resources

Here are some additional resources for various topics:

- Calculus Fundamentals
  - [Essence of Calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) by Grant Sanderson
- Linear Algebra Fundamentals
  - [Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) by Grant Sanderson
- Books
  - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - Free online book
  - [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow and Yoshua Bengio and Aaron Courville
- Visualizations
  - [Neural Network Playground](https://playground.tensorflow.org/) - A playground for dense neural networks
  - [Gan Lab](https://poloclub.github.io/ganlab/) - A playground for GANs
  - [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/) - Visual tutorial on initialization in deep learning
  - [Parameter optimization in neural networks](https://www.deeplearning.ai/ai-notes/optimization/) - Visual tutorial on optimization in deep learning
- Stanford Courses
  - [CS 230 Deep Learning](https://cs230.stanford.edu/)
  - [CS 231N Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
  - [CS 224N Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
  - [CS 236 Deep Generative Models](https://deepgenerativemodels.github.io/)
- Interesting talks on advanced topics
  - Ben Recht - [Training on test set and other heresies](https://www.youtube.com/watch?v=NTz4rJS9BAI)
  - Aleksander Madry - [A new perspective on Adversarial Perturbations](https://www.youtube.com/watch?v=mUt7w4UoYqM) -->
