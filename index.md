---
layout: default
---

# Workshop Description

Through a series of rapid surveys, including guest lectures, we will present an overview of recent topics in deep learning and machine learning with particular relevance for practitioners. Areas will include embeddings and dimensionality reduction, transfer learning, representation learning, weakly supervised / semi-supervised / self-supervised and active learning. This workshop will assume a familiarity with basic concepts from both machine learning and deep learning as taught in the introductory workshops on those topics, but it will not assume a deep statistical background. Prior experience with applying neural networks is highly recommended.

_Prerequisites:_ Familiarity of basic concepts from linear algebra, such as vectors and matrices, as well as calculus concepts, such as differentiation. Familiarity with the python programming language and an ability to use Jupyter notebooks will be helpful for the hands-on sessions.

## About the Instructors

![Sherrie Wang](/assets/img/sherrie.png){:style="max-width:30%;"}

Sherrie Wang graduated from Stanford in 2021 with a PhD in Computational and Mathematical Engineering and is now a Ciriacy-Wantrup Postdoctoral Fellow at UC Berkeley. She works on developing machine learning methods for remote sensing applications, especially in settings where ground truth labels are scarce. These methods are then applied to problems in sustainable agriculture and development, such as mapping where crops are grown in developing countries.

![Alexander Ioannidis](/assets/img/alex.png){:style="max-width:30%;"}

Alexander Ioannidis earned his Ph.D. in Computational and Mathematical Engineering and Masters in Management Science and Engineering both at Stanford University. He is a research fellow working on developing novel machine learning techniques for medical and genomic applications in the Department of Biomedical Data Science. Prior to Stanford he earned his bachelors in Chemistry and Physics from Harvard and a M.Phil from the University of Cambridge. He conducted research for several years on novel superconducting and quantum computing architectures at Northrop Grumman's Advanced Technologies research center. In his free time, he enjoys sailing.

# Workshop Materials

## Pre-workshop Checklist

1. Sign up for [Piazza](http://piazza.com/icme/summer2021/icme) (class code *icme*). We will be using Piazza to answer questions during the workshop.
2. You should have received a welcome email with the Zoom link and password.  Please email us (tanishj@stanford.edu) if you haven't.
3. Familiarize yourself with the schedule and see you Monday August 16th at 8:00 am PCT!

## Schedule

#### Monday August 16th
TBA
<!-- - Session 1 (8:00 AM to 9:30 AM)
  - Introduction
  - Current state of the art in deep learning
  - Math review
  - Architecture of multi-layer neural networks
- Session 2 (9:30 AM to 11:00 AM)
  - Loss functions
  - The backpropgation algorithm
  - The gradient descent algorithm
  - Walkthrough of an example -->

#### Tuesday August 17th
TBA
<!-- - Session 3 (8:00 AM to 9:30 AM)
  - Over-fitting and Under-fitting
  - Convolutional Neural Networks
  - Recurrent Neural Networks
- Session 4 (9:30 AM to 11:00 AM)
  - Other Architectures
  - Deep Learning Libraries
  - Walkthrough of an example
  - Failures of deep learning -->

## Slides

* Session 1 - [slides](/docs/dlworkshop2021_1.pdf)
* Session 2 - [slides](/docs/dlworkshop2021_2.pdf)

<!-- ## Jupyter Notebooks 

Below links should open the notebooks in [Google Collaboratory](https://colab.research.google.com/), after they open you may have to click "Open in Playground" to be able to run code.

*Links coming soon!*

* [TFWalkthrough.ipynb](https://colab.research.google.com/drive/1yGCtmXoN-bvFpOvcwxE5TJ2lu4WSyPAB)
* [KerasWalkthrough.ipynb](https://colab.research.google.com/drive/1uX27nH7K7UUn0RoQ0mREZ6FSiTv7F4TJ)
* [TransferLearning.ipynb](https://colab.research.google.com/drive/1QrNPyIalL4_i8aMO6426GV40dk3anPwJ) -->


## Additional Resources

Here are some additional resources for various topics:

- Calculus Fundamentals
  - [Essence of Calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) by Grant Sanderson
- Linear Algebra Fundamentals
  - [Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) by Grant Sanderson
- Books
  - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - Free online book
  - [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow and Yoshua Bengio and Aaron Courville
- Visualizations
  - [Neural Network Playground](https://playground.tensorflow.org/) - A playground for dense neural networks
  - [Gan Lab](https://poloclub.github.io/ganlab/) - A playground for GANs
  - [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/) - Visual tutorial on initialization in deep learning
  - [Parameter optimization in neural networks](https://www.deeplearning.ai/ai-notes/optimization/) - Visual tutorial on optimization in deep learning
- Stanford Courses
  - [CS 230 Deep Learning](https://cs230.stanford.edu/)
  - [CS 231N Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
  - [CS 224N Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
  - [CS 236 Deep Generative Models](https://deepgenerativemodels.github.io/)
- Interesting talks on advanced topics
  - Ben Recht - [Training on test set and other heresies](https://www.youtube.com/watch?v=NTz4rJS9BAI)
  - Aleksander Madry - [A new perspective on Adversarial Perturbations](https://www.youtube.com/watch?v=mUt7w4UoYqM)
